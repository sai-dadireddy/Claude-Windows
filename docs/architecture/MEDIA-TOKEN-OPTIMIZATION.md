# Media Token Optimization Guide

**Created**: 2025-10-13
**Purpose**: Optimize token usage for YouTube transcripts and images
**Critical**: Media can consume 20K-50K tokens if not managed!

---

## ğŸ¯ The Problem

You've optimized:
- âœ… MCP overhead: 66.8K â†’ 8K
- âœ… Instructions: 20K â†’ 3.5K
- âœ… Startup: 155K â†’ 40K

But media content can quickly negate these savings:
- YouTube transcripts: 2K-12K tokens per video
- Images: 1K-1.5K tokens per HD screenshot
- Research session: Easily 20K-50K tokens of media!

---

## ğŸ“Š Token Costs Reference

### **YouTube Transcripts**
| Video Length | Words | Tokens (with formatting) |
|--------------|-------|--------------------------|
| 10 minutes   | 1,500 | ~2,000 tokens |
| 30 minutes   | 4,500 | ~6,000 tokens |
| 60 minutes   | 9,000 | ~12,000 tokens |

**Real scenarios**:
```
Researching AI news (5 videos, 20-min each): ~20K tokens
Tutorial deep-dive (3 videos, 45-min each): ~27K tokens
Conference talk (1 video, 90-min): ~18K tokens
```

### **Images**
| Image Size | Use Case | Token Cost |
|------------|----------|------------|
| 200x200 | Small icon/logo | ~85 tokens |
| 500x500 | Medium diagram | ~255 tokens |
| 1000x1000 | Large diagram | ~765 tokens |
| 1920x1080 (HD) | Screenshot | ~1,200-1,400 tokens |
| 2000x2000 | High-res diagram | ~1,600 tokens |

**Real scenarios**:
```
Architecture review (10 diagrams): ~7,000-8,000 tokens
UI debugging (5 screenshots): ~6,000-7,000 tokens
Presentation analysis (15 slides): ~12,000-15,000 tokens
```

---

## ğŸš€ Strategy 1: Agent-Based Media Processing (BEST!)

### **Create YouTube Research Agent**

File: `.claude/agents/youtube-research-heavy.md`

```markdown
---
name: "YouTube Research with Transcript Analysis"
description: "Isolated agent for YouTube transcript extraction and analysis"
model: "sonnet"
---

# YouTube Research Agent (Isolated Context)

You are a specialized YouTube research agent.

## Context Isolation
- Own 200K context window
- Process transcripts in YOUR context, not main
- Return summaries only (2-5K tokens max)

## Workflow
1. Extract YouTube transcript (2K-12K tokens in YOUR context)
2. Analyze and identify key points
3. Create structured summary:
   - Main topics (bullet points)
   - Key insights (numbered list)
   - Relevant quotes (with timestamps)
   - Actionable takeaways
4. Store full transcript in memory-auto (for later reference)
5. Return ONLY summary to main session

## What to Return
```json
{
  "video_title": "...",
  "duration": "30 minutes",
  "main_topics": [...],
  "key_insights": [...],
  "relevant_quotes": [
    {"timestamp": "5:32", "quote": "...", "context": "..."}
  ],
  "actionable_takeaways": [...],
  "full_transcript_stored": "memory_id_12345"
}
```

## Compression Ratio
Aim for 10:1 compression:
- 30-min video (6K tokens) â†’ Summary (600 tokens)
- Main session never sees full transcript!
```

**Usage**:
```
You: "Use youtube-research-heavy to analyze this tutorial: [URL]"

Agent:
â”œâ”€ Isolated 200K context
â”œâ”€ Loads transcript: 6K tokens (in agent context)
â”œâ”€ Analyzes content
â”œâ”€ Stores full transcript in memory
â””â”€ Returns: 600-token summary to main

Main session: 40K + 0.6K = 40.6K tokens âœ…
(vs 40K + 6K = 46K if loaded directly!)
```

---

### **Create Visual Analysis Agent**

File: `.claude/agents/visual-analysis-heavy.md`

```markdown
---
name: "Visual Analysis with Image Processing"
description: "Isolated agent for image analysis and processing"
model: "sonnet"
---

# Visual Analysis Agent (Isolated Context)

You are a specialized visual analysis agent.

## Context Isolation
- Own 200K context window
- Process images in YOUR context
- Return structured analysis only

## Workflow
1. Receive images (1K-1.5K tokens EACH in YOUR context)
2. Analyze visual content:
   - Architecture diagrams â†’ Extract components and relationships
   - Screenshots â†’ Identify UI elements and issues
   - Flowcharts â†’ Convert to text/mermaid diagram
   - Code screenshots â†’ OCR and extract text
3. Create structured output (text-based)
4. Return analysis to main session (NO images!)

## What to Return
For architecture diagrams:
```markdown
## Architecture Analysis

### Components Identified:
- Frontend: React app with Redux state management
- API Gateway: Express.js on port 3000
- Database: PostgreSQL with connection pooling
- Cache: Redis for session storage

### Data Flow:
1. User request â†’ Frontend
2. Frontend â†’ API Gateway (REST)
3. API Gateway â†’ Database (SQL queries)
4. Response cached in Redis
5. Response â†’ Frontend

### Key Observations:
- No load balancer shown
- Single point of failure at API Gateway
- Database not replicated
```

For screenshots:
```markdown
## UI Analysis

### Layout:
- Header: Logo left, nav right
- Main content: 3-column grid
- Sidebar: Filters and search

### Issues Identified:
1. Button text cutoff on mobile (line 45, Button.tsx)
2. Misaligned icons in nav (CSS issue)
3. Color contrast fails WCAG (background #eee, text #ccc)

### Recommendations:
- Add responsive breakpoints at 768px and 1024px
- Increase text color to #666 for contrast
- Fix button padding to prevent text overflow
```

## Compression
- 1 HD screenshot (1.4K tokens) â†’ Text analysis (300 tokens)
- 10 screenshots (14K tokens) â†’ Analysis (3K tokens)
- Compression: 78% token savings!
```

**Usage**:
```
You: "Use visual-analysis-heavy to review these architecture diagrams"
[Attach 10 images]

Agent:
â”œâ”€ Isolated 200K context
â”œâ”€ Processes 10 images: 8K tokens (in agent context)
â”œâ”€ Extracts components, relationships, flows
â”œâ”€ Creates text-based analysis
â””â”€ Returns: 2K text analysis to main

Main session: 40K + 2K = 42K tokens âœ…
(vs 40K + 8K = 48K if images loaded directly!)
```

---

## ğŸš€ Strategy 2: Memory-Based Caching

### **Store Full Transcripts in Memory**

```
User: "Research these 3 YouTube videos"

Workflow:
1. youtube-research-heavy agent extracts transcripts
2. Agent stores FULL transcripts in memory-auto MCP:
   - video_1_full_transcript (6K tokens in memory, NOT context)
   - video_2_full_transcript (8K tokens in memory)
   - video_3_full_transcript (5K tokens in memory)
3. Agent returns summaries only (2K total to main)
4. Main session: 40K + 2K = 42K tokens âœ…

Later:
User: "What did video 2 say about authentication at 15:30?"

Claude:
1. Queries memory for video_2_full_transcript
2. Retrieves relevant 2-minute section (~200 tokens)
3. Answers question using context from memory

No need to load full 8K transcript into main session!
```

**Benefits**:
- Full transcripts available for reference
- Only load what's needed when needed
- Main context stays lean

---

## ğŸš€ Strategy 3: Progressive Detail Loading

### **Start Minimal, Add Detail as Needed**

```
Level 1: Summary only (600 tokens)
â”œâ”€ User: "What's this video about?"
â””â”€ Response: Use summary from agent

Level 2: Specific sections (200-500 tokens per section)
â”œâ”€ User: "What did they say about authentication?"
â””â”€ Load relevant 2-minute transcript section from memory

Level 3: Full transcript (6K tokens)
â”œâ”€ User: "I need to quote extensively from this"
â””â”€ Load full transcript from memory (only when necessary!)
```

**Example**:
```
Session start: 40K tokens

User: "Research 5 videos on Claude Code optimization"

You:
â”œâ”€ Spawn youtube-research-heavy agent
â”œâ”€ Agent extracts 5 transcripts (30K tokens in agent context)
â”œâ”€ Agent stores full transcripts in memory
â”œâ”€ Agent returns 5 summaries (3K tokens to main)
â””â”€ Main: 40K + 3K = 43K tokens âœ…

User: "What did video 3 say about MCP optimization?"

You:
â”œâ”€ Query memory for video_3 transcript
â”œâ”€ Retrieve "MCP optimization" section (500 tokens)
â”œâ”€ Main: 43K + 0.5K = 43.5K tokens âœ…

User: "Give me exact quotes with timestamps for my article"

You:
â”œâ”€ Load full video_3 transcript from memory (6K tokens)
â”œâ”€ Main: 43.5K + 6K = 49.5K tokens
â””â”€ Still under 50K! âœ…
```

---

## ğŸš€ Strategy 4: Image Optimization Guidelines

### **Before Loading Images**

1. **Ask if needed**:
   ```
   User: "Here's a screenshot of the error"

   Claude: "Can you describe the error message text?
   I can help without seeing the image, which saves tokens."
   ```

2. **Request optimized size**:
   ```
   Claude: "If you need to share the screenshot:
   - Crop to relevant area only
   - Resize to max 1000x1000
   - Use PNG (not BMP or uncompressed formats)"
   ```

3. **Extract text first**:
   ```
   For code screenshots:
   - OCR the text
   - Share code as text (much fewer tokens!)
   - Image only if formatting/context matters
   ```

### **Image Processing Workflow**

```
User shares HD screenshot (1.4K tokens)

Option A (Inefficient):
â”œâ”€ Load image into main context: 1.4K tokens
â”œâ”€ Claude analyzes
â””â”€ Main: 40K + 1.4K = 41.4K tokens

Option B (Efficient - Use Agent):
â”œâ”€ visual-analysis-heavy agent receives image
â”œâ”€ Image loaded in agent context: 1.4K (isolated)
â”œâ”€ Agent extracts text/analysis: 300 tokens
â””â”€ Main: 40K + 0.3K = 40.3K tokens âœ…

Savings: 1.1K tokens per image!
10 images: 11K tokens saved!
```

---

## ğŸš€ Strategy 5: Batch Processing

### **Process Multiple Media Items Together**

```
Instead of:
â”œâ”€ Load video 1 transcript (6K)
â”œâ”€ Discuss, then unload
â”œâ”€ Load video 2 transcript (8K)
â”œâ”€ Discuss, then unload
â”œâ”€ Load video 3 transcript (5K)
â””â”€ Total: 19K tokens loaded sequentially

Do this:
â”œâ”€ Spawn youtube-research-heavy agent
â”œâ”€ Agent processes all 3 videos in isolated context (19K in agent)
â”œâ”€ Agent creates comparative analysis
â”œâ”€ Agent returns: "Video 1 covered X, Video 2 focused on Y, Video 3 added Z"
â””â”€ Main receives: 2K summary covering all 3 videos âœ…
```

---

## ğŸ“‹ Practical Examples

### **Example 1: AI News Research**

**Bad Approach**:
```
Morning: Load 5 AI news video transcripts (25K tokens)
Session: 40K + 25K = 65K tokens (32.5% usage)
After discussion: 65K + 30K work = 95K tokens (47.5%)
```

**Good Approach**:
```
Morning: youtube-research-heavy processes 5 videos
â”œâ”€ Agent context: 25K tokens (isolated)
â”œâ”€ Agent returns: 3K summary to main
â””â”€ Main: 40K + 3K = 43K tokens (21.5%)

Throughout day:
â”œâ”€ Reference specific videos via memory queries
â”œâ”€ Add 30K work context
â””â”€ End of day: 73K tokens (36.5%)

Savings: 22K tokens (23% less context usage!)
```

---

### **Example 2: Architecture Review**

**Bad Approach**:
```
Load 10 architecture diagrams: 8K tokens
Discuss architecture: 20K tokens
Total: 40K + 8K + 20K = 68K tokens
```

**Good Approach**:
```
visual-analysis-heavy analyzes 10 diagrams:
â”œâ”€ Agent context: 8K tokens (isolated)
â”œâ”€ Agent extracts: Components, relationships, flows
â”œâ”€ Agent returns: 2K text-based analysis
â””â”€ Main: 40K + 2K = 42K tokens

Discuss architecture based on text analysis: 20K
Total: 42K + 20K = 62K tokens

Savings: 6K tokens per architecture review!
```

---

## ğŸ¯ Best Practices Summary

### **YouTube Transcripts**
1. âœ… Always use youtube-research-heavy agent
2. âœ… Store full transcripts in memory
3. âœ… Work from summaries (10:1 compression)
4. âœ… Load specific sections only when needed
5. âŒ Never load full transcripts into main context

### **Images**
1. âœ… Ask if description would work first
2. âœ… Use visual-analysis-heavy agent for batches
3. âœ… Optimize image size (max 1000x1000)
4. âœ… Extract text from code screenshots
5. âŒ Never load full-res images into main context

### **General Media**
1. âœ… Process in agent contexts (isolated)
2. âœ… Store full content in memory
3. âœ… Return summaries/analysis only
4. âœ… Load details progressively as needed
5. âŒ Never accumulate media in main session

---

## ğŸ“Š Token Savings Calculator

### **Scenario: Daily Research Session**

**Without Optimization**:
```
Startup: 40K
+ 5 YouTube videos: +25K
+ 10 architecture diagrams: +8K
+ Work context: +30K
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total: 103K tokens (51.5%)
```

**With Optimization**:
```
Startup: 40K
+ YouTube summaries (agent): +3K
+ Diagram analysis (agent): +2K
+ Work context: +30K
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total: 75K tokens (37.5%)

Savings: 28K tokens (27% reduction!)
```

---

## ğŸš€ Quick Start

### **1. Create YouTube Research Agent**
Copy `.claude/agents/youtube-research-heavy.md` template above

### **2. Create Visual Analysis Agent**
Copy `.claude/agents/visual-analysis-heavy.md` template above

### **3. Update Your Workflow**
```
Old: "Analyze this YouTube video: [URL]"
New: "Use youtube-research-heavy to analyze: [URL]"

Old: [Attach 10 screenshots]
New: "Use visual-analysis-heavy to review these images"
```

### **4. Use Memory for Storage**
```
Full transcripts/images â†’ Store in memory-auto
Query memory when you need specific details
Keep main context lean!
```

---

## ğŸ† Success Metrics

Your media optimization is working when:
- âœ… YouTube research: +3K tokens (not +25K)
- âœ… Image analysis: +2K tokens (not +8K)
- âœ… Daily sessions: <80K tokens (not 100K+)
- âœ… Never exceeding 50% context in normal work
- âœ… All-day sessions without restart

---

**Combined with MCP + Instruction optimization:**
- MCP: 66.8K â†’ 8K âœ…
- Instructions: 20K â†’ 3.5K âœ…
- Media: 33K â†’ 5K âœ… (with agents)
- **Total savings: 143K tokens (71.5% reduction!)**

**You can now research 5 videos with 10 images and still be under 50K tokens!** ğŸ‰
